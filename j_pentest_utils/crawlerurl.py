#!/usr/bin/env python
# encoding: utf8

"""
universal crawler url handle

"""

import os
import json
import random
import requests
import urlparse
import traceback
import Queue
import threading
import tldextract


class Url( object ):
    """
        hashable url( with naive hash algorithm )
    """
    def __init__( self, url ):
        self.url = url

    def _key( self ):
        u = urlparse.urlparse( self.url )

        path = u.path # todo: more precise key over path

        q = tuple( urlparse.parse_qs( u.query ).keys() )

        return ( u.netloc, u.path, q )

    def __hash__( self ):
        return hash( self._key() )


class CrawlerUrl(object):
    """
        put_url: store url in this
        get_url: get one url

        CrawlerUrl will automaticly check if url crawled
    """

    def __init__( self, allowed_tld, queue_max=2**20 ):
        self.allowed_tld = allowed_tld
        self.url_queue = Queue.Queue()
        self.queue_max = queue_max
        self.url_set = set()
        self.lock = threading.Lock()

    def put_url( self, url ):
        """
            put url only if not met before
        """
        if not self.check_tld( url ):
            return

        urlhash = hash( Url(url) )

        if urlhash not in self.url_set:
            with self.lock:
                self.url_queue.put( url )
                self.url_set.add( urlhash )

    def put_url_list( self, url_list ):
        for url in url_list:
            self.put_url( url )

    def get_url( self ):
        with self.lock:
            url = self.url_queue.get()
        return url

    def check_tld( self, url ):
        if not self.allowed_tld: # allowed_tld not set, crawl any url
            return True
        tld = tldextract.extract( url ).registered_domain
        return tld in self.allowed_tld

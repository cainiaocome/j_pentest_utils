#!/usr/bin/env python
# encoding: utf8

"""
universal crawler url handle

"""

import os
import json
import random
import requests
#import urllib.parse
import urllib.parse
import traceback
import queue
import threading
import tldextract
from collections import Counter

# todo
# move crawlerurl to a separate package
# use j_pentest_url

class Url( object ):
    """
        hashable url( with naive hash algorithm )
    """
    def __init__( self, url ):
        self.url = url

    def _key( self ):
        u = urllib.parse.urllib.parse( self.url )

        p = tuple( filter( None, u.path.split( '/' ) ) )
        if len(p)==0:
            pass
        elif len(p)==1:
            p = ( p[0][:2], )
        else:
            p = p[:-1]

        q = tuple( urllib.parse.parse_qs( u.query ).keys() )

        return ( u.netloc, p, q )

    def __hash__( self ):
        return hash( self._key() )


class CrawlerUrl(object):
    """
        put_url: store url in this
        get_url: get one url

        CrawlerUrl will automaticly check if url crawled
    """

    def __init__( self, allowed_tld, queue_max=2**20 ):
        self.allowed_tld = allowed_tld
        self.url_queue = queue.Queue()
        self.queue_max = queue_max
        self.url_hash_set = set()
        self.url_counter = Counter()  # 每次向url_queue添加一次记录一次
        self.lock = threading.Lock()

    def put_url( self, url ):
        """
            put url only if not met before
        """
        if not self.check_tld( url ):
            return

        urlhash = hash( Url(url) )

        if urlhash not in self.url_hash_set:
            """
                如果urlhash不在url_hash_set里面，就直接添加
            """
            with self.lock:
                self.url_queue.put( url )
                self.url_counter.update( [ urlhash, ] )
                self.url_hash_set.add( urlhash )
        else:
            """
                如果urlhash在url_hash_set里面，就按算法一定概率添加
            """
            urlhash_counter = self.url_counter.get( urlhash )
            if random.choice( range(urlhash_counter) )==0:
                self.url_queue.put( url )
                self.url_counter.update( [ urlhash, ] )

    def put_url_list( self, url_list ):
        for url in url_list:
            self.put_url( url )

    def get_url( self ):
        with self.lock:
            url = self.url_queue.get()
        return url

    def check_tld( self, url ):
        if not self.allowed_tld: # allowed_tld not set, crawl any url
            return True
        tld = tldextract.extract( url ).registered_domain
        return tld in self.allowed_tld

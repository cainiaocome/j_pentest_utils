#!/usr/bin/env python
# encoding: utf8

"""
universal crawler url handle

"""

import os
import json
import random
import requests
import urlparse
import traceback
import Queue
import threading
import tldextract
from collections import Counter


class Url( object ):
    """
        hashable url( with naive hash algorithm )
    """
    def __init__( self, url ):
        self.url = url

    def _key( self ):
        u = urlparse.urlparse( self.url )

        p = tuple( filter( None, u.path.split( '/' ) ) )
        if len(p)==0:
            pass
        elif len(p)==1:
            p = ( p[0][:2], )
        else:
            p = p[:-1]

        q = tuple( urlparse.parse_qs( u.query ).keys() )

        return ( u.netloc, p, q )

    def __hash__( self ):
        return hash( self._key() )


class CrawlerUrl(object):
    """
        put_url: store url in this
        get_url: get one url

        CrawlerUrl will automaticly check if url crawled
    """

    def __init__( self, allowed_tld, queue_max=2**20 ):
        self.allowed_tld = allowed_tld
        self.url_queue = Queue.Queue()
        self.queue_max = queue_max
        self.url_set = set()
        self.url_counter = Counter()  # 每次向url_queue添加一次记录一次
        self.lock = threading.Lock()

    def put_url( self, url ):
        """
            put url only if not met before
        """
        if not self.check_tld( url ):
            return

        urlhash = hash( Url(url) )

        if urlhash not in self.url_set:
            with self.lock:
                self.url_queue.put( url )
                self.url_counter.update( [ urlhash, ] )
                self.url_set.add( urlhash )
        else:
            urlhash_counter = self.url_counter.get( urlhash )
            if random.choice( xrange(urlhash_counter) )==0:
                self.url_queue.put( url )
                self.url_counter.update( [ urlhash, ] )

    def put_url_list( self, url_list ):
        for url in url_list:
            self.put_url( url )

    def get_url( self ):
        with self.lock:
            url = self.url_queue.get()
        return url

    def check_tld( self, url ):
        if not self.allowed_tld: # allowed_tld not set, crawl any url
            return True
        tld = tldextract.extract( url ).registered_domain
        return tld in self.allowed_tld
